{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichalSlowakiewicz/Statistical-Data-Analysis-2/blob/master/LAB_10_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SAD2 lab 10**\n",
        "\n",
        "Adapted from [Navarro, Danielle.](https://orcid.org/0000-0001-7648-6578) 2023. “The Metropolis-Hastings Algorithm.” April 12, 2023. https://blog.djnavarro.net/posts/2023-04-12_metropolis-hastings/. (*Licensed under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)*)"
      ],
      "metadata": {
        "id": "nIFS5bZxsu_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem formulation\n",
        "\n",
        "The **Metropolis-Hastings** algorithm is perhaps the most popular example of a **Markov chain Monte Carlo (MCMC)** method in statistics. The basic problem that it solves is to provide a method for sampling from some arbitrary probability distribution $\\mathbb{P}(X=x)$ with probability density function denoted as $f(x)$. The idea is that in many cases, you know how to write out the equation for the probability , but you don't know how to generate a random number from this distribution, . This is the situation where MCMC is handy. For example, suppose we've become interested for reasons known but to the gods in the probability distribution shown below:\n",
        "\n",
        "![MH_distribution.jpg](data:image/jpeg;base64,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)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Um-KbWn2s2yC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The probability density function $f(x)$ for this distribution is given by the following equation:\n",
        "\n",
        "$$f(x) =  \\frac{\\exp(-x^2)\\Bigl(2 +\\sin(5x) + (\\sin(2x)\\Bigl)}{\\int_{-∞}^{∞}\\exp(-u^2)\\Bigl(2 +\\sin(5u) + (\\sin(2u)\\Bigl) \\mathrm{d} u}$$\n",
        "\n",
        "The problem might occur if we don't know how to solve the integral in the denominator analytically or in case of high-dimentional distributions brute force numerical methods to compute integral might not work ([Curse of dimentionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)).\n",
        "\n",
        "So this means in truth, we only know the distribution \"up to some unknown constant\". That is, all we really know how to calculate is the numerator. Given this, a more realistic way to express our knowledge about the target distribution is captured by this equation:\n",
        "\n",
        "$$f(x) \\propto \\exp(-x^2)\\Bigl(2 +\\sin(5x) + (\\sin(2x)\\Bigl)$$\n",
        "\n",
        "But how can I generate samples from this distribution?"
      ],
      "metadata": {
        "id": "DMMXe_fxCU58"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Metropolis-Hastings algorithm\n",
        "\n",
        "The basic idea behind MCMC is very simple: define a [Markov chain](https://en.wikipedia.org/wiki/Markov_chain) over possible $x$ values, in such a way that the **stationary distribution** of the Markov chain is in fact searched probability density function $f(x)$. We will use a Markov chain to generate a sequence of $x$ values, denoted $(x_0, x_1, \\dots, x_n)$, in such a way that as $n \\to \\infty$, it is guaranteed that $x(n) \\sim f(x)$. There are multiple different ways of setting up a Markov chain having this property. One of them is **Metropolis-Hastings algorithm**.\n",
        "\n",
        "## The proposal step\n",
        "\n",
        "First let's remeber that $\\{X_0, X_1, \\dots, X_n\\}$ is a sequence of random variables in a Markov chain and $x_0, x_1, \\dots, x_n$ is an observed sequence of states of respective random variables.\n",
        "\n",
        "Now, suppose that the current state of the Markov chain is $x_n$. We want to generate the next state in the chain $x_{n+1}$. In the Metropolis-Hastings algorithm, the generation of $x_{n+1}$ is a two-stage process.\n",
        "\n",
        "The first stage is to generate a **candidate**, which we'll denote $x^*$. The value of $x^*$ is generated from the **proposal distribution** we can sample from. We denote this proposal distribution $q(x^*|x_n)$. Notice that the distribution we sample from depends on the current state of the Markov chain, $x_n$. There are some technical constraints on what you can use as a proposal distribution, but for the most part it can be anything you like, however, the sampler won't work if there are some values $x$ that never get proposed. The standard way to do this is to use a normal distribution centered on the current state $x_n$. More formally, we write this as:\n",
        "\n",
        "$$x^* | x_n \\sim \\text{Normal}(x_n, \\sigma^2) $$\n",
        "\n",
        "for some standard deviation $\\sigma$ that we select in advance (more on this later!)\n",
        "\n",
        "\n",
        "## The accept-reject step\n",
        "\n",
        "The second stage is the accept-reject step. Firstly, we calculate the **acceptance probability**, denoted $A(x_n \\to x^*)$, which is given by:\n",
        "\n",
        "$$A(x_n \\to x^*) = \\min\\Bigl(1, \\frac{f(x^*)}{f(x_n)} \\times \\frac{q(x_n | x^*)}{q(x^*|x_n)}\\Bigl).$$\n",
        "\n",
        "First, let's notice that the ratio $\\frac{f(x^*)}{f(x^n)}$ does not depend on the normalising constant for the distribution, which is the integral we usually can't easily compute (not in our case). We have\n",
        "\n",
        "$$\\frac{f(x^*)}{f(x^n)} = \\frac{\\exp(-x^{*2})(2 +\\sin(5x^*) + (\\sin(2x^*))}{\\exp(-x_n^2)(2 +\\sin(5x_n) + (\\sin(2x_n))}$$\n",
        "\n",
        "We can compute above equality without the need for numerical integration or solving intergral analytically.\n",
        "\n",
        "Another thing worth noting is the behaviour of term, $\\frac{q(x_n | x^*)}{q(x^*|x_n)}$.\n",
        "What this term does is correct for any biases that the proposal distribution might induce. In this expression, the denominator $q(x^*|x_n)$ describes the probability with which we would choose $x^*$ as the candidate if the current state of the Markov chain is $x_n$. The numerator, however, describes the probability of a transition that goes the other way: that is, if the current state had actually been $x^*$, what is the probability that you would have generated $x_n$ as the candidate value? If the proposal distribution is *symmetric*, which normal distribution is, then these two probabilities will turn out to be equal. So for normal distribution we will have:\n",
        "\n",
        "$$\\begin{align*}q(x^*|x_n) &= \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\Bigl(\\frac{1}{2\\sigma^2}(x_n - x^*)^2\\Bigl),\\\\\n",
        "q(x_n|x^*) &= \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\Bigl(\\frac{1}{2\\sigma^2}(x^* - x_n)^2\\Bigl).\n",
        "\\end{align*}$$\n",
        "\n",
        "Clearly, $q(x^*|x_n) = q(x_n|x^*)$\n",
        "for all choices of $x_n$ and $x^*$, and as a consequence\n",
        "\n",
        "$$\\frac{q(x_n | x^*)}{q(x^*|x_n)} = 1.$$\n",
        "\n",
        "This special case of the Metropolis-Hastings algorithm, in which the proposal distribution is symmetric, is referred to as the **Metropolis algorithm**.\n",
        "\n",
        "Having proposed the candidate $x^*$\n",
        "and calculated the acceptance probability, $A(x_n \\to x^*)$, we now either decide to \"accept\" the candidate and set $x_{n+1} = x^*$ or we \"reject\" the candidate and set $x_{n+1} = x_n$. To make this decision, we generate a uniformly distributed random number between $0$ and $1$, denoted $u$. We can write this criterion as:\n",
        "\n",
        "$$x_{n+1} = \\begin{cases}x^* \\quad \\text{if } u \\leq A(x_n \\to x^*) \\\\\n",
        "x_n \\quad \\text{otherwise}.\\end{cases}$$\n",
        "\n",
        "It is important to note that this is just simple example of Metropolis-Hastings algorithm (Metropolis algorithm to be specific) and there are quite a few technical issues that need to be explored to actually use the algorithm in real world setting, so further reading is necessary. This example is suppose to show some intuition about how the MCMC works."
      ],
      "metadata": {
        "id": "AmJQNcCWDLHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing the sampler\n",
        "\n",
        "\n",
        "### Exercise 1: Implementation\n",
        "\n",
        "Let's implement Metropolis-Hastings algorithm by writing function that draws samples from previously defined distribution. First define the sampler in terms of three functions:\n",
        "\n",
        "1. `target`- calculates the probability of a given sample $x$ (more precisely: the numerator term in the equation describing the probability density),\n",
        "\n",
        "2. `metropolis_step` - takes some value $x$ corresponding to the current state of the Markov chain and a parameter `sigma` that describes the standard deviation of the proposal distribution,\n",
        "\n",
        "3. `metropolis_sampler` - runs the Metropolis algorithm for `n` number of steps and has following parameters\n",
        "\n",
        "  *  `initial_value` - start point for MC,\n",
        "  * `n` - number of samples drawn from target distribution,\n",
        "  * `sigma` - standard deviation of normal distribution used for proposing candidate values at each step,\n",
        "  * `burnin` - *burn in period* - number if iterations of sampler run before recording results\n",
        "  * `lag` - number of iterations run between recording succesive samples.\n",
        "\n",
        "Functions in 2 and 3 should return dictionary with `value` recording the numeric value that vas sampled and `accepted` logical variable indicating whether this value arose from accepting the proposal (rather than rejecting it)."
      ],
      "metadata": {
        "id": "oa4tZg3yDXxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3cjWRF5upkMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def target(x):\n",
        "    #############################################\n",
        "    res = ...\n",
        "    ##############################################\n",
        "    return res\n",
        "\n",
        "def metropolis_step(x, sigma):\n",
        "\n",
        "    ##############################################\n",
        "    # Propose a new value for x from a normal distribution\n",
        "\n",
        "    # Calculate the acceptance probability\n",
        "\n",
        "    # Draw a random uniform number\n",
        "\n",
        "    # Decide whether to accept the proposed x\n",
        "\n",
        "    #############################################\n",
        "\n",
        "    # Return the result as a dictionary\n",
        "    out = {'value': value, 'accepted': accepted}\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "tzce61q7K5Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metropolis_sampler(initial_value, n=1000, sigma=1, burnin=0, lag=0):\n",
        "    results = []\n",
        "    current_state = initial_value\n",
        "\n",
        "    # Burn-in phase\n",
        "    for _ in range(burnin):\n",
        "      #######################################################\n",
        "      # TODO\n",
        "      #####################################################\n",
        "\n",
        "    # Sampling phase\n",
        "    for _ in range(n):\n",
        "        for _ in range(1+lag):\n",
        "          ###################################################\n",
        "          # TODO\n",
        "          ###################################################\n",
        "        results.append(out)\n",
        "\n",
        "    # Convert results into a torch tensor for the final output\n",
        "    values = torch.tensor([r['value'].item() for r in results])\n",
        "    accepted = torch.tensor([r['accepted'] for r in results])\n",
        "\n",
        "    # Return the results in a dictionary\n",
        "    return {'values': values, 'accepted': accepted}"
      ],
      "metadata": {
        "id": "e5Omg4sPLEHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's run `metropolis_sampler`."
      ],
      "metadata": {
        "id": "_XJikpjLGKYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "# Run the sampler with initial_value = 0\n",
        "out = metropolis_sampler(initial_value=torch.tensor(0.0))\n",
        "\n",
        "# Print the first 10 values (both value and accepted status)\n",
        "for i in range(10):\n",
        "    print(f\"Value: {out['values'][i].item()}, Accepted: {out['accepted'][i].item()}\")"
      ],
      "metadata": {
        "id": "IDYxBbC5Mjbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The effect of proposal width\n",
        "\n",
        "\n",
        "When implementing a Metropolis algorithm, the choice of proposal distribution is crutial. If it's too narrow, the sampler will have an extremely high acceptance rate on average, but it will move around extremely slowly. To use the technical term, it has a very low *mixing rate*. This distorts our estimate of the target distribution. However, if the proposal distribution is too wide, the acceptance rate becomes too low and the chain gets stuck on specific values for long periods of time. This also distorts the estimate of the target distribution.\n",
        "\n",
        "Yes, technically, if we run our accursed sampler long enough despite the poor choice of proposal distribution it will eventually produce the right answer. Nevertheless, we would prefer a sampler that gives the right answer quickly rather than slowly.\n",
        "\n",
        "### Exercise 2: Effect of proposal with\n",
        "\n",
        "1. Run Metropolis sampler with `n = 10000` on our problem three times, each with different `sigma` value: `1, 0.025, 50`.\n",
        "\n",
        "2. Plot histogram of sample values.\n",
        "\n",
        "3. Plot the Markov chain itself: sample values at each time-step. It's ususally reffered to as *trace plot*.\n",
        "\n",
        "3. Interpret the insights we can gather about proposal distribution from the plots."
      ],
      "metadata": {
        "id": "vsEDcx93Qp2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "# Adjust plot layout settings\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "plt.subplots_adjust(hspace=0.4, wspace=0.3)\n",
        "\n",
        "# Parameters\n",
        "######################################################################\n",
        "sigma = ...\n",
        "n = ...\n",
        "######################################################################\n",
        "\n",
        "# Generate plots for each sigma\n",
        "for i in range(3):\n",
        "    # Run the Metropolis sampler\n",
        "    out = metropolis_sampler(initial_value=torch.tensor(-1.0), n=n, sigma=sigma[i])\n",
        "\n",
        "    # Extract values\n",
        "    values = out['values'].numpy()\n",
        "\n",
        "    # Histogram\n",
        "    ax_hist = axes[0, i]\n",
        "    ax_hist.hist(values, bins=np.arange(-3, 3, 0.05), color='skyblue', edgecolor='black')\n",
        "    ax_hist.set_title(f\"Sample values: sigma = {sigma[i]}\")\n",
        "    ax_hist.set_xlabel(\"Value, x\")\n",
        "    ax_hist.set_ylabel(\"Frequency\")\n",
        "\n",
        "    ######################################################################\n",
        "    # Overlay expected distribution\n",
        "    x = np.arange(-3, 3, 0.05)\n",
        "    p = ...  # Calculate target distribution\n",
        "    expected = ...  # Expected values based on target distribution\n",
        "    ax_hist.plot(x, expected, color='red', linewidth=2)\n",
        "    ######################################################################\n",
        "\n",
        "    # Time-series plot\n",
        "    ax_plot = axes[1, i]\n",
        "    ax_plot.plot(values, np.arange(1, n + 1), color='blue')\n",
        "    ax_plot.set_xlim([-3, 3])\n",
        "    ax_plot.set_xlabel(\"Value, x\")\n",
        "    ax_plot.set_ylabel(\"Time\")\n",
        "\n",
        "# Display the plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oB4y-_mPQAmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notes for tutors**: The way to read these plots is as follows: for all three values of `sigma`, we have two plots. The top one shows a histogram of the samples values obtained using the Metropolis sampler (that's the black bars). Superimposed on this is a red line showing the distribution of values you'd expect to obtain when sampling from the true distribution. The lower panel plots the Markov chain itself: the sequence of generated values.9\n",
        "\n",
        "In the leftmost plots, we see what happens when we choose a good proposal distribution: the chain shown in the lower panel moves rapidly across the whole distribution, without getting stuck in any one place. In the far right panel, we see what happens when the proposal distribution is too wide: the chain gets stuck in one spot for long periods of time. It does manage to make big jumps, covering the whole range, but because the acceptance rate is so low that the distribution of samples is highly irregular. Finally, in the middle panel, if we set the proposal distribution to be too narrow, the acceptance rate is very high so the chain doesn't get stuck in any one spot, but it doesn't cover a very wide range. This simple example should give you an intuition for why you need to “play around” with the choice of proposal distribution. A good proposal distribution can make a huge difference!"
      ],
      "metadata": {
        "id": "sy8qthvZIA0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The role of burn-in period\n",
        "\n",
        "Up to this point we haven't really explained what the `burnin` and `lag` parameters are there for. We won't go into details but present only basic idea. First, let's think about the *burn-in* issue.\n",
        "\n",
        "### Exercise 3: Burn-in period\n",
        "\n",
        "Suppose we started the sampler at a very bad location… say `initial_value = -3`, and - just so that we can exaggerate the problem - we'll use a proposal distribution that is too narrow, say `sigma = .1`.\n",
        "\n",
        "1. Run sampler three times with `n=1000`, `initial_value = -3` and `sigma = .1` and plot each of the runs similarily as the in the Exercise 2.\n",
        "\n",
        "2. Interpret the results. How does the Markov chain behave, after what step the behaviour changes? Add line to the plot where you see the difference in behaviour.\n",
        "\n",
        "3. What happends in the beggining of the run? What is the impact of the \"bad\" start location? What is location of the histograms with respect to original distribution.\n",
        "\n",
        "Now let's try to fix the issues observed in the previous points. To do this we will let the algorithm run for a while before starting to collect actual samples. The length of time that you spend doing this is called the **burn in period**.\n",
        "\n",
        "4. Set the parameter `burnin` to the value found in the point 2. Run the algorithm.\n",
        "\n",
        "5. Interpret the results. What happened? Did the results improve? What other problems you can observe? What might be reason (hint: think obaout other parameters)?"
      ],
      "metadata": {
        "id": "sTLCpnmvQv61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "# Adjust plot layout settings\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "plt.subplots_adjust(hspace=0.4, wspace=0.3)\n",
        "\n",
        "######################################################################\n",
        "# Parameters\n",
        "n = ...\n",
        "######################################################################\n",
        "\n",
        "# Generate plots\n",
        "for i in range(3):\n",
        "    ######################################################################\n",
        "    # Run the Metropolis sampler\n",
        "    out = ...\n",
        "\n",
        "    # Extract values\n",
        "    values = out['values'].numpy()\n",
        "    ######################################################################\n",
        "\n",
        "    # Histogram\n",
        "    ax_hist = axes[0, i]\n",
        "    ax_hist.hist(values, bins=np.arange(-4, 4, 0.05), color='skyblue', edgecolor='black')\n",
        "    ax_hist.set_xlabel(\"Value, x\")\n",
        "    ax_hist.set_ylabel(\"Frequency\")\n",
        "\n",
        "    ######################################################################\n",
        "    # Overlay expected distribution\n",
        "    x = np.arange(-4, 4, 0.05)\n",
        "    p = ...  # Calculate target distribution\n",
        "    expected = ...  # Expected values based on target distribution\n",
        "    ax_hist.plot(x, expected, color='red', linewidth=2)\n",
        "    ######################################################################\n",
        "\n",
        "    # Time-series plot\n",
        "    ax_plot = axes[1, i]\n",
        "    ax_plot.plot(values, np.arange(1, n + 1), color='blue')\n",
        "    ax_plot.set_xlim([-4, 4])\n",
        "    ax_plot.set_xlabel(\"Value, x\")\n",
        "    ax_plot.set_ylabel(\"Time\")\n",
        "\n",
        "    # Add horizontal dotted line at y=200\n",
        "    ######################################################################\n",
        "    # TODO\n",
        "    ######################################################################\n",
        "\n",
        "# Display the plots\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FwX19fgvQQyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notes for tutors**: As you can see, the sampler spends the first 200 or so iterations slowly moving rightwards towards the main body of the distribution. Once it gets there, the samples start to look okay, but notice that the histograms are biased towards the left (i.e., towards the bad start location)."
      ],
      "metadata": {
        "id": "lZcYrVNeJDVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "# Adjust plot layout settings\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "plt.subplots_adjust(hspace=0.4, wspace=0.3)\n",
        "\n",
        "######################################################################\n",
        "# Parameters\n",
        "n = 1000\n",
        "######################################################################\n",
        "\n",
        "# Generate plots\n",
        "for i in range(3):\n",
        "    # Run the Metropolis sampler with burnin\n",
        "    out = metropolis_sampler(initial_value=torch.tensor(-3.0), burnin=200, n=n, sigma=0.1)\n",
        "\n",
        "    # Extract values\n",
        "    values = out['values'].numpy()\n",
        "\n",
        "    # Histogram\n",
        "    ax_hist = axes[0, i]\n",
        "    ax_hist.hist(values, bins=np.arange(-4, 4, 0.05), color='skyblue', edgecolor='black')\n",
        "    ax_hist.set_xlabel(\"Value, x\")\n",
        "    ax_hist.set_ylabel(\"Frequency\")\n",
        "\n",
        "    ######################################################################\n",
        "    # Overlay expected distribution\n",
        "    x = np.arange(-4, 4, 0.05)\n",
        "    p = ...  # Calculate target distribution\n",
        "    expected = ...  # Expected values based on target distribution\n",
        "    ax_hist.plot(x, expected, color='red', linewidth=2)\n",
        "    ######################################################################\n",
        "\n",
        "    # Time-series plot\n",
        "    ax_plot = axes[1, i]\n",
        "    ax_plot.plot(values, np.arange(1, n + 1), color='blue')\n",
        "    ax_plot.set_xlim([-4, 4])\n",
        "    ax_plot.set_xlabel(\"Value, x\")\n",
        "    ax_plot.set_ylabel(\"Time\")\n",
        "\n",
        "# Display the plots\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Mfm-bpdeR2V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note for tutors**: It's still not ideal - largely because we don't have many samples, we haven't set a lag, and the value of sigma isn't very well chosen - but you can see that the bias caused by the poor choice of starting value has disappeared."
      ],
      "metadata": {
        "id": "rqKKEqx5JJwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The role of lag parameter\n",
        "\n",
        "Finally, we'll mention in passing the role played by the `lag` parameter, which is used in so-called *thinning phase*. In some situations we can be forced into using a proposal distribution that has a very low acceptance rate. When that happens, we're left with an awkward Markov chain that gets stuck in one location for long periods of time. One thing that people often do in that situation is allow several iterations of the sampler to elapse in between successive samples. This is the `lag` between samples.\n",
        "\n",
        "### Exercise 4: Thinning phase\n",
        "\n",
        "Illustrate the effect of the thinning phase for the sampler with a very wide proposal distribution (`sigma = 50`).\n",
        "\n",
        "1. Run sampler three times with `n = 1000` samples drawn, `sigma = 50`, and different `lag` values: `0, 10, 100`.\n",
        "\n",
        "2. Interpret the results. What is the link between [autocorrelation](https://en.wikipedia.org/wiki/Autocorrelation) of the succesive samples and lag? Do we want our samples to be independent, why or why not?"
      ],
      "metadata": {
        "id": "zBNGUfxXQ31z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "# Adjust plot layout settings\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "plt.subplots_adjust(hspace=0.4, wspace=0.3)\n",
        "\n",
        "######################################################################\n",
        "# Parameters\n",
        "n = 1000\n",
        "lag_values = [0, 10, 100]\n",
        "######################################################################\n",
        "\n",
        "# Generate plots for different lags\n",
        "for i in range(3):\n",
        "    ######################################################################\n",
        "    # Run the Metropolis sampler with specified lag\n",
        "    out = ...\n",
        "\n",
        "    # Extract values\n",
        "    values = out['values'].numpy()\n",
        "    ######################################################################\n",
        "\n",
        "    # Histogram\n",
        "    ax_hist = axes[0, i]\n",
        "    ax_hist.hist(values, bins=np.arange(-4, 4, 0.05), color='skyblue', edgecolor='black')\n",
        "    ax_hist.set_title(f\"Lag: {lag_values[i]}\")\n",
        "    ax_hist.set_xlabel(\"Value, x\")\n",
        "    ax_hist.set_ylabel(\"Frequency\")\n",
        "\n",
        "    ######################################################################\n",
        "    # Overlay expected distribution\n",
        "    x = np.arange(-3, 3, 0.05)\n",
        "    p = ...  # Calculate target distribution\n",
        "    expected = ...  # Expected values based on target distribution\n",
        "    ax_hist.plot(x, expected, color='red', linewidth=2)\n",
        "    ######################################################################\n",
        "\n",
        "    # Time-series plot\n",
        "    ax_plot = axes[1, i]\n",
        "    ax_plot.plot(values, np.arange(1, n + 1), color='blue')\n",
        "    ax_plot.set_xlim([-4, 4])\n",
        "    ax_plot.set_xlabel(\"Value, x\")\n",
        "    ax_plot.set_ylabel(\"Time\")\n",
        "\n",
        "# Display the plots\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZDp9pjaASMIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note for tutors**: Formally speaking, the thing we're trying to do by increasing `lag` is reduce the [autocorrelation](https://en.wikipedia.org/wiki/Autocorrelation) between successive samples in our chain. In an ideal world we want our sampled values to be independent samples from the target distribution $f(x)$. The more our samples are correlated with each other, the more potential there is for the histogram of sampled values to depart systematically from the target distribution. Introducing a lag between successive samples is a simple way to achieve this."
      ],
      "metadata": {
        "id": "7cTlnPyaJ3wA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reminder\n",
        "\n",
        "The discussion is heavily oversimplified. It doesn't discuss the conditions required to make Metropolis sampling work, it doesn't talk about diagnostics, and it certainly doesn't talk about what happens when we move this into higher dimensional problems."
      ],
      "metadata": {
        "id": "CnjB1EzJKVBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Another example - sampling from posterior distribution\n",
        "\n",
        "*Maria Bochenek*\n",
        "\n",
        "## Model setup\n",
        "\n",
        "Suppose we have observation $X \\mid \\theta \\sim \\mathcal{N}(\\theta, 1)$ with Cauchy prior on the mean, $\\theta \\sim \\text{Cauchy}(0,1)$.\n",
        "\n",
        "We would like to perform update our beliefs about $\\theta$ based on the information provided by data $X$ - obtain posterior distribution of $\\theta$ given $X$.\n",
        "\n",
        "The likelihood is\n",
        "\n",
        "$$\n",
        "L(x \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left[ -\\frac{1}{2} (x - \\theta)^2\\right],\n",
        "$$\n",
        "\n",
        "and the prior follows [Cauchy distribution](https://en.wikipedia.org/wiki/Cauchy_distribution) with location parameter equal $0$ and scale parameter equal $1$ and can be expressed as\n",
        "\n",
        "$$\n",
        "f(\\theta; 0, 1) = \\frac{1}{\\pi(1 + \\theta^2)}.\n",
        "$$\n",
        "\n",
        "The posterior distribution is proportional to the likelihood times prior, that is\n",
        "\n",
        "$$\n",
        "f(\\theta \\mid x) \\propto \\exp\\left[-\\frac{1}{2}(x - \\theta)^2\\right] \\frac{1}{1 + \\theta^2},\n",
        "$$\n",
        "\n",
        "with the normalizing constant (probability of producing the data) being the reciprocal of\n",
        "\n",
        "$$\n",
        "Z(x) = \\int_{-\\infty}^{\\infty}  \\exp\\left[-\\frac{1}{2}(x - \\theta)^2\\right]  \\frac{1}{1 + \\theta^2} \\mathrm{d} \\theta.\n",
        "$$\n",
        "\n",
        "In practice this normalizing constant is treated as intractable and not evaluated analytically. Instead Monte Carlo methods such as Metropolis Hastings algorithm are used to generate samples from such distribution and perform Bayesian inference.\n",
        "\n",
        "Now, suppose we have observed data $X = (x_1, \\dots, x_n)$, then the posterior probability will be proportional to\n",
        "\n",
        "$$\n",
        "f(\\theta \\mid X) \\propto  \\frac{1}{1 + \\theta^2} \\prod_{i=1}^n \\exp\\left[-\\frac{1}{2}(x_i - \\theta)^2\\right]\n",
        "$$\n",
        "\n",
        "As usual let's work with log-probabilities. We have\n",
        "\n",
        "$$\n",
        "\\log f(\\theta \\mid X) \\propto - \\log\\left(1 + \\theta^2\\right) -\\frac{1}{2} \\sum_{i=1}^n (x_i - \\theta)^2.\n",
        "$$\n"
      ],
      "metadata": {
        "id": "-BEFzizduXj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 5\n",
        "\n",
        "Perform Metropolis-Hastings algorithm on the example above.\n",
        "\n",
        "1. Use logarithm of the numerator of posterior distribution as the target distribution (note that it is proportional to posterior). Fill in the gaps in `log_target` function.\n",
        "\n",
        "1. Adapt the code for `metropolis_step` function, so that it"
      ],
      "metadata": {
        "id": "AT44bbDdB9d6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate some data - normally this would be empirically collected\n",
        "\n",
        "torch.manual_seed(1234)\n",
        "theta_true = 1.5\n",
        "n = 100\n",
        "X = torch.normal(theta_true, 1, size=(n,))"
      ],
      "metadata": {
        "id": "ux4USFPqFY7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_target(theta, data):\n",
        "    ##############################################\n",
        "    res = ...\n",
        "    ##############################################\n",
        "    return res\n",
        "\n",
        "def metropolis_step(theta, data, sigma):\n",
        "\n",
        "    ##############################################\n",
        "    # Propose a new value for theta from a normal distribution - random walk\n",
        "\n",
        "    # Calculate the acceptance probability\n",
        "\n",
        "    # Draw a random uniform number\n",
        "\n",
        "    # Decide whether to accept the proposed theta\n",
        "\n",
        "    #############################################\n",
        "\n",
        "    # Return the result as a dictionary\n",
        "    out = {'value': value, 'accepted': accepted}\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "XPubf6X6D_fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metropolis_sampler(initial_value, data, n=1000, sigma=1, burnin=0, lag=0):\n",
        "    results = []\n",
        "    current_state = initial_value\n",
        "\n",
        "    # Burn-in phase\n",
        "    for _ in range(burnin):\n",
        "      #######################################################\n",
        "      # TODO\n",
        "      #####################################################\n",
        "\n",
        "    # Sampling phase\n",
        "    for _ in range(n):\n",
        "        for _ in range(1+lag):\n",
        "          ###################################################\n",
        "          # TODO\n",
        "          ###################################################\n",
        "        results.append(out)\n",
        "\n",
        "    # Convert results into a torch tensor for the final output\n",
        "    values = torch.tensor([r['value'].item() for r in results])\n",
        "    accepted = torch.tensor([r['accepted'] for r in results])\n",
        "\n",
        "    # Return the results in a dictionary\n",
        "    return {'values': values, 'accepted': accepted}"
      ],
      "metadata": {
        "id": "WKtWQz9SDvXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "# Run the sampler with initial_value = 0\n",
        "out = metropolis_sampler(initial_value=torch.tensor(0.0), x=X)\n",
        "\n",
        "# Print the first 10 values (both value and accepted status)\n",
        "for i in range(10):\n",
        "    print(f\"Value: {out['values'][i].item()}, Accepted: {out['accepted'][i].item()}\")"
      ],
      "metadata": {
        "id": "nOui7PvDD0Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "# Adjust plot layout settings\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "plt.subplots_adjust(hspace=0.4, wspace=0.3)\n",
        "\n",
        "# Parameters\n",
        "######################################################################\n",
        "sigma = [0.001, 1.0, 100]\n",
        "n = 10000\n",
        "######################################################################\n",
        "\n",
        "# Generate plots for each sigma\n",
        "for i in range(3):\n",
        "    # Run the Metropolis sampler\n",
        "    out = metropolis_sampler(initial_value=torch.tensor(0.0),\n",
        "                             x=X,\n",
        "                             n=n,\n",
        "                             sigma=sigma[i],\n",
        "                             burnin=200,\n",
        "                            #  lag=10,\n",
        "                             )\n",
        "\n",
        "    # Extract values\n",
        "    values = out['values'].numpy()\n",
        "\n",
        "    # Histogram\n",
        "    ax_hist = axes[0, i]\n",
        "    ax_hist.hist(values, bins=np.arange(-1, 3, 0.05), color='skyblue', edgecolor='black')\n",
        "    ax_hist.set_title(f\"Sample values: sigma = {sigma[i]}\")\n",
        "    ax_hist.set_xlabel(r\"Value, $\\theta$\")\n",
        "    ax_hist.set_ylabel(\"Frequency\")\n",
        "\n",
        "    # ######################################################################\n",
        "    # Overlay expected distribution\n",
        "    thetas = np.arange(-1, 3, 0.05)\n",
        "    p = torch.tensor([torch.exp(target(torch.tensor(y), X)) for y in thetas]) # Calculate target distribution\n",
        "    expected = n * p / p.sum()  # Expected values based on target distribution\n",
        "    ax_hist.plot(thetas, expected, color='red', linewidth=2)\n",
        "    # ######################################################################\n",
        "\n",
        "    # Time-series plot\n",
        "    ax_plot = axes[1, i]\n",
        "    ax_plot.plot(values, np.arange(1, n + 1), color='blue')\n",
        "    ax_plot.set_xlim([-1, 3])\n",
        "    ax_plot.set_xlabel(r\"Value, $\\theta$\")\n",
        "    ax_plot.set_ylabel(\"Time\")\n",
        "\n",
        "# Display the plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PphOOOv8JGLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "# Adjust plot layout settings\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "plt.subplots_adjust(hspace=0.4, wspace=0.3)\n",
        "\n",
        "# Parameters\n",
        "######################################################################\n",
        "sigma = 1.0\n",
        "n = 10000\n",
        "lag_values = [0, 10, 100]\n",
        "######################################################################\n",
        "\n",
        "# Generate plots for each sigma\n",
        "for i in range(3):\n",
        "    # Run the Metropolis sampler\n",
        "    out = metropolis_sampler(initial_value=torch.tensor(0.0),\n",
        "                             x=X,\n",
        "                             n=n,\n",
        "                             sigma=sigma,\n",
        "                             burnin=200,\n",
        "                             lag=lag_values[i],\n",
        "                             )\n",
        "\n",
        "    # Extract values\n",
        "    values = out['values'].numpy()\n",
        "\n",
        "    # Histogram\n",
        "    ax_hist = axes[0, i]\n",
        "    ax_hist.hist(values, bins=np.arange(-1, 3, 0.05), color='skyblue', edgecolor='black')\n",
        "    ax_hist.set_title(f\"Lag = {lag_values[i]}\")\n",
        "    ax_hist.set_xlabel(r\"Value, $\\theta$\")\n",
        "    ax_hist.set_ylabel(\"Frequency\")\n",
        "\n",
        "    # ######################################################################\n",
        "    # Overlay expected distribution\n",
        "    thetas = np.arange(-1, 3, 0.05)\n",
        "    p = torch.tensor([torch.exp(target(torch.tensor(y), X)) for y in thetas]) # Calculate target distribution\n",
        "    expected = n * p / p.sum()  # Expected values based on target distribution\n",
        "    ax_hist.plot(thetas, expected, color='red', linewidth=2)\n",
        "    # ######################################################################\n",
        "\n",
        "    # Time-series plot\n",
        "    ax_plot = axes[1, i]\n",
        "    ax_plot.plot(values, np.arange(1, n + 1), color='blue')\n",
        "    ax_plot.set_xlim([-1, 3])\n",
        "    ax_plot.set_xlabel(r\"Value, $\\theta$\")\n",
        "    ax_plot.set_ylabel(\"Time\")\n",
        "\n",
        "# Display the plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TvWgr502YwwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xHYn623lKwIx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}