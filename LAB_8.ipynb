{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichalSlowakiewicz/Statistical-Data-Analysis-2/blob/master/LAB_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SAD2, lab 8:** Expectation-Maximization for Hidden Markov Models\n",
        "\n",
        "*Justyna Król*\n",
        "\n"
      ],
      "metadata": {
        "id": "VvGFWrp347n8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Expectation–Maximization (EM) Algorithm**\n",
        "\n",
        "In many real-world problems we want to estimate model parameters even though part of the data is hidden or unobserved. Examples include clustering, mixture models, gene annotation, and Hidden Markov Models (HMMs). In such settings, the likelihood function becomes difficult to optimize directly because we must marginalize over all possible latent variable configurations.\n",
        "\n",
        "Let:\n",
        "\n",
        "$X$ = observed data\n",
        "\n",
        "$Z$ = latent (unobserved) variables\n",
        "\n",
        "$\\theta$ = model parameters we want to estimate\n",
        "\n",
        "We want to maximize the log-likelihood:\n",
        "\n",
        "$$ \\log p(X \\mid \\theta) = \\log \\sum_{Z} p(X, Z \\mid \\theta) $$\n",
        "\n",
        "Direct maximization is often intractable because of the summation over $Z$.\n",
        "EM solves this using two alternating steps:\n",
        "\n",
        "#### **1) E-Step (Expectation)**\n",
        "\n",
        "Compute $ Q(\\theta \\mid \\theta^{(t)})$ defined as the expected value of the log likelihood function of  $\\theta$, with respect to the current conditional distribution of  $Z$ given  $X$ and the current estimates of the parameters $\\theta^{(t)}$:\n",
        "\n",
        "$$\n",
        "Q(\\theta \\mid \\theta^{(t)})\n",
        "= \\mathbb{E}_{Z \\sim p(\\cdot \\mid X, \\theta^{(t)})} \\left[ \\log p(X, Z \\mid \\theta) \\right]\n",
        ":= \\int \\log p(X, Z \\mid \\theta) \\, p(Z \\mid X, \\theta^{(t)}) \\, dZ .\n",
        "$$\n",
        "\n",
        "\n",
        "#### **2) M-Step (Maximization)**\n",
        "\n",
        "Update parameters by maximizing the $Q$ function:\n",
        "\n",
        "$$ \\theta^{(t+1)} = \\arg\\max_{\\theta} Q(\\theta \\mid \\theta^{(t)}) $$\n",
        "\n",
        "This gives us the new parameters - then the EM cycle repeats until convergence.\n"
      ],
      "metadata": {
        "id": "wdaIipm_d-ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reminder: Hidden Markov Models\n",
        "\n",
        ">*Definition.*\n",
        "A **Hidden Markov Model** $\\mathcal{M}$ is a triplet  \n",
        "$$\n",
        "\\mathcal{M} = (\\Sigma, Q, \\Theta),\n",
        "$$\n",
        "where:\n",
        "\n",
        "- $\\Sigma$ is an alphabet of observable symbols,  \n",
        "- $Q$ is a finite set of hidden states,  \n",
        "- $\\Theta = (\\pi, T, E)$ is a collection of probability distributions consisting of:  \n",
        "\n",
        "  - **initial state probabilities** $\\pi_i$ for each $i \\in Q$:\n",
        "  $$ \\pi_i = \\mathbb{P}(Z_1 = i) $$\n",
        "\n",
        "  - **transition probabilities** $t_{i,j}$ for $i, j \\in Q$:\n",
        "  $$ t_{i,j} = \\mathbb{P}(Z_n = j \\mid Z_{n-1} = i) $$\n",
        "\n",
        "  - **emission probabilities** $e_j(s)$ for $j \\in Q$ and $s \\in \\Sigma$:\n",
        "  $$ e_j(s) = \\mathbb{P}(X_n = s \\mid Z_n = j) $$\n",
        "\n",
        "Here,  \n",
        "- $X = (X_1, X_2, \\ldots)$ is the sequence of observable symbols over $\\Sigma$, and  \n",
        "- $Z = (Z_1, Z_2, \\ldots)$ is the sequence of hidden states over $Q$.\n"
      ],
      "metadata": {
        "id": "f4fiqatExS_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EM for HMMs\n",
        "\n",
        "**Motivation:**  \n",
        "If the parameters $\\theta$ of an HMM are known, the distribution over the latent states $Z$ can be computed efficiently using the **forward–backward algorithm**.  \n",
        "Conversely, if the latent states $Z$ were known, estimating the parameters $\\theta$ would be straightforward: we could count how often transitions and emissions occur and normalize these counts to obtain updated transition, emission, and initial probabilities.\n",
        "\n",
        "This motivates an iterative procedure when both $\\theta$ and $Z$ are unknown:\n",
        "\n",
        "1. Initialize the parameters $\\theta$.\n",
        "2. **E-step:** Using the forward–backward algorithm, compute the posterior distribution of the latent states $Z$ given the current parameters $\\theta$.\n",
        "3. **M-step:** Update $\\theta$ using the expected sufficient statistics of $Z$ computed in the E-step.\n",
        "4. Repeat steps 2–3 until convergence.\n",
        "\n",
        "This EM procedure guarantees a monotonic increase in the data log-likelihood and converges to a local optimum.\n"
      ],
      "metadata": {
        "id": "OvaGjR0cwHHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Baum–Welch Algorithm**\n",
        "\n",
        "The **Baum–Welch algorithm** is an EM procedure for HMMs that iteratively updates parameters $\\theta = (\\pi, T, E)$ when the latent states $Z$ are unknown.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. **E-step:** Compute the posterior probabilities of hidden states using the **forward–backward algorithm**:\n",
        "\n",
        "$$\n",
        "\\gamma_n(i) = \\mathbb{P}(Z_n = i \\mid X, \\theta), $$\n",
        "$$\n",
        "\\xi_n(i,j) = \\mathbb{P}(Z_n = i, Z_{n+1} = j \\mid X, \\theta)\n",
        "$$\n",
        "\n",
        "2. **M-step:** Update the parameters using these expected counts:\n",
        "\n",
        "- Initial state probabilities:\n",
        "$$\n",
        "\\pi_i^{\\text{new}} = \\gamma_1(i)\n",
        "$$\n",
        "\n",
        "- Transition probabilities:\n",
        "$$\n",
        "t_{ij}^{\\text{new}} = \\frac{\\sum_{n=1}^{N-1} \\xi_n(i,j)}{\\mathbb{P}(X)}\n",
        "$$\n",
        "\n",
        "- Emission probabilities:\n",
        "$$\n",
        "e_j(s)^{\\text{new}} = \\frac{\\sum_{n=1}^{N} \\gamma_n(j) \\mathbf{1}_{\\{X_n = s\\}}}{\\mathbb{P}(X)}\n",
        "$$\n",
        "\n",
        "Repeat E-step and M-step until convergence. The algorithm monotonically increases the data log-likelihood and converges to a local optimum.\n"
      ],
      "metadata": {
        "id": "uumVeU-l1d2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Theoretical task 1:**\n",
        "\n",
        "Let $X_1, \\dots, X_N$ be a sequence of observations in an HMM.  \n",
        "Given:  \n",
        "\n",
        "- the current parameter estimates $\\theta = (\\pi, T, E)$,  \n",
        "- the output of the forward function $f_{i, n} = \\mathbb{P}(X_{1:n}, Z_n = i )$,  \n",
        "- the output of the backward function $b_{i, n} = \\mathbb{P}(X_{n+1:N} \\mid Z_n = i)$,  \n",
        "\n",
        "prove that the joint posterior of consecutive hidden states is\n",
        "\n",
        "$$\n",
        "\\xi_n(i,j) = \\mathbb{P}(Z_n = i, Z_{n+1} = j \\mid X) =  \\frac{f_{i,n} \\, t_{ij} \\, e_j(X_{n+1}) \\, b_{j,n+1}}{\\mathbb{P}(X)}\n",
        "$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hGIXl7At2cEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Log Probabilities in Forward, Backward, and Baum–Welch Algorithms\n",
        "\n",
        "In this lab, we will work with **log probabilities** to improve numerical stability when dealing with long sequences. Accordingly, the **forward, backward, and Baum–Welch algorithms** will be implemented in a log-space version.  \n",
        "\n",
        "- In the **forward algorithm**, we compute the log of the forward variables $\\log f_{i,n}$, replacing products of probabilities with sums of logs.  \n",
        "- Similarly, the **backward algorithm** uses $\\log b_{i,n}$ to propagate probabilities backward in log-space.  \n",
        "- The **Baum–Welch EM updates** are also computed using log probabilities, ensuring that expected counts and parameter updates remain numerically stable.  \n",
        "\n",
        "Using log-space computations prevents underflow, allows handling long sequences, and ensures that all steps of the EM procedure are robust.\n"
      ],
      "metadata": {
        "id": "jSXLKuT1_q3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baum–Welch Algorithm for Multiple Sequences\n",
        "\n",
        "The standard Baum–Welch algorithm works on a single observation sequence, but it can be generalized to handle **multiple independent sequences** $X^{(1)}, X^{(2)}, \\dots, X^{(M)}$ generated from the same HMM.\n",
        "\n",
        "**Algorithm Steps:**\n",
        "\n",
        "1. **E-step:**  \n",
        "   For each sequence $X^{(m)}$, compute the forward and backward variables and the expected counts of:  \n",
        "\n",
        "   - Transitions:\n",
        "   $$\n",
        "   \\xi_n^{(m)}(i,j) = \\mathbb{P}(Z_n = i, Z_{n+1} = j \\mid X^{(m)}, \\theta)\n",
        "   $$\n",
        "\n",
        "   - States:\n",
        "   $$\n",
        "   \\gamma_n^{(m)}(i) = \\mathbb{P}(Z_n = i \\mid X^{(m)}, \\theta)\n",
        "   $$\n",
        "\n",
        "2. **M-step:**  \n",
        "   Update the HMM parameters by **summing over all sequences**:\n",
        "\n",
        "   $$\n",
        "   \\pi_i^{\\text{new}} = \\frac{1}{M} \\sum_{m=1}^M \\gamma_1^{(m)}(i)\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   t_{ij}^{\\text{new}} = \\frac{\\sum_{m=1}^M \\sum_{n=1}^{N_m-1} \\xi_n^{(m)}(i,j)}{\\sum_{m=1}^M \\sum_{n=1}^{N_m-1} \\gamma_n^{(m)}(i)}\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   e_j(s)^{\\text{new}} = \\frac{\\sum_{m=1}^M \\sum_{n=1}^{N_m} \\gamma_n^{(m)}(j) \\mathbf{1}_{\\{X_n^{(m)} = s\\}}}{\\sum_{m=1}^M \\sum_{n=1}^{N_m} \\gamma_n^{(m)}(j)}\n",
        "   $$\n",
        "\n",
        "3. **Iterate** the E-step and M-step until convergence.\n",
        "\n",
        "**Notes:**\n",
        "\n",
        "- Each sequence contributes independently to the expected counts.  \n",
        "- Summing over sequences ensures that parameter updates are informed by all observed data.  \n",
        "- The log-likelihood of the full dataset (sum over sequences) increases monotonically at each iteration.\n"
      ],
      "metadata": {
        "id": "nj--zDqocTYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercises**\n",
        "\n",
        "You are provided with a working implementation of the multi-sequence **Baum–Welch algorithm** for Hidden Markov Models. Your goal is to examine the algorithm's convergence behavior.\n",
        "\n",
        "First, examine the code below."
      ],
      "metadata": {
        "id": "9gEItjYBReH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "def forward_log(T_log, E_log, pi_log, sequence):\n",
        "    \"\"\"\n",
        "    Forward algorithm in log space.\n",
        "    T_log: log transition matrix (K x K)\n",
        "    E_log: log emission matrix  (K x M)\n",
        "    pi_log: log initial probabilities (K)\n",
        "    sequence: observed sequence (list or tensor of ints)\n",
        "    \"\"\"\n",
        "    K = E_log.shape[0]\n",
        "    N = len(sequence)\n",
        "\n",
        "    f_log = torch.zeros((K, N), dtype=torch.float32)\n",
        "\n",
        "    # Initialization\n",
        "    f_log[:, 0] = pi_log + E_log[:, sequence[0]]\n",
        "\n",
        "    # Recursion\n",
        "    for i in range(1, N):\n",
        "        for l in range(K):\n",
        "            f_log[l, i] = E_log[l, sequence[i]] + torch.logsumexp(f_log[:, i-1] + T_log[:, l], dim=0)\n",
        "\n",
        "    loglik = torch.logsumexp(f_log[:, -1], dim=0)\n",
        "\n",
        "    return f_log, loglik\n",
        "\n",
        "\n",
        "def backward_log(T_log, E_log, sequence):\n",
        "    \"\"\"\n",
        "    Backward algorithm in log space.\n",
        "    T_log: log transition matrix (K x K)\n",
        "    E_log: log emission matrix (K x M)\n",
        "    sequence: observed sequence\n",
        "    \"\"\"\n",
        "    K = T_log.shape[0]\n",
        "    N = len(sequence)\n",
        "\n",
        "    b_log = torch.zeros((K, N), dtype=torch.float32)\n",
        "\n",
        "    # Initialization\n",
        "    b_log[:, N - 1] = 0.\n",
        "\n",
        "    for i in range(N - 2, -1, -1):\n",
        "        for k in range(K):\n",
        "            b_log[k, i] = torch.logsumexp(T_log[k, :] + E_log[:, sequence[i + 1]] + b_log[:, i + 1], dim=0)\n",
        "\n",
        "    return b_log\n",
        "\n",
        "\n",
        "def get_posterior_log(T_log, E_log, pi_log, sequence):\n",
        "    \"\"\"\n",
        "    Compute posterior p(z_i | x) from log forward and log backward scores\n",
        "    Posterior is returned in ordinary prob space.\n",
        "    \"\"\"\n",
        "    f_log, loglik = forward_log(T_log, E_log, pi_log, sequence)\n",
        "    b_log = backward_log(T_log, E_log, sequence)\n",
        "\n",
        "    log_posterior = f_log + b_log\n",
        "\n",
        "    # Convert to normalized probabilities\n",
        "    log_posterior -= torch.logsumexp(log_posterior, dim=0, keepdim=True)\n",
        "\n",
        "    posterior = torch.exp(log_posterior)\n",
        "    return posterior\n",
        "\n",
        "\n",
        "def baum_welch_log(sequences, K, M, init_T, init_E, init_pi, max_iter=50, tol=1e-12):\n",
        "    \"\"\"\n",
        "    Baum–Welch training for multiple sequences using log-space forward/backward.\n",
        "\n",
        "    sequences: list of int tensors (each length N_n)\n",
        "    K: number of hidden states\n",
        "    M: number of emission symbols\n",
        "    \"\"\"\n",
        "    T_log = torch.log(init_T + 1e-12)\n",
        "    E_log = torch.log(init_E + 1e-12)\n",
        "    pi_log = torch.log(init_pi + 1e-12)\n",
        "\n",
        "    last_loglik = -float(\"inf\")\n",
        "    logliks = []\n",
        "    for iteration in range(max_iter):\n",
        "        # Accumulators for expected counts\n",
        "        gamma_sum_for_transitions = torch.zeros((K,))\n",
        "        gamma_init_sum = torch.zeros((K,))\n",
        "        emission_sum = torch.zeros((K, M))\n",
        "        xi_sum = torch.zeros((K, K))\n",
        "\n",
        "        total_loglik = 0.0\n",
        "\n",
        "        # E-step: calculate log_likelihood\n",
        "        for seq in sequences:\n",
        "            # Forward / Backward in log space\n",
        "            f_log, loglik = forward_log(T_log, E_log, pi_log, seq)\n",
        "            b_log = backward_log(T_log, E_log, seq)\n",
        "\n",
        "            total_loglik += loglik.item()\n",
        "\n",
        "            # Posterior state probabilities\n",
        "            gamma_log = f_log + b_log - loglik\n",
        "            gamma = torch.exp(gamma_log)         # (K × N)\n",
        "\n",
        "            # Posterior transition probabilities: xi\n",
        "            N = len(seq)\n",
        "            for i in range(N - 1):\n",
        "                xi_log = (\n",
        "                    f_log[:, i].unsqueeze(1)\n",
        "                    + T_log\n",
        "                    + E_log[:, seq[i + 1]].unsqueeze(0)\n",
        "                    + b_log[:, i + 1].unsqueeze(0)\n",
        "                    - loglik\n",
        "                )\n",
        "                xi = torch.exp(xi_log)  # (K × K)\n",
        "                xi_sum += xi\n",
        "\n",
        "            # Accumulate expected counts\n",
        "            gamma_sum_for_transitions += torch.sum(gamma[:, :-1], dim=1)\n",
        "            gamma_init_sum += gamma[:, 0]\n",
        "            for i, x in enumerate(seq):\n",
        "                emission_sum[:, x] += gamma[:, i]\n",
        "\n",
        "        # M-step: update probabilities\n",
        "        pi = gamma_init_sum / torch.sum(gamma_init_sum)\n",
        "        T = xi_sum / gamma_sum_for_transitions.unsqueeze(1)\n",
        "        E = emission_sum / torch.sum(emission_sum, dim=1, keepdim=True)\n",
        "\n",
        "        # Convert to log-space for next iteration\n",
        "        T_log = torch.log(T + 1e-12)\n",
        "        E_log = torch.log(E + 1e-12)\n",
        "        pi_log = torch.log(pi + 1e-12)\n",
        "\n",
        "        # Check convergence\n",
        "        if iteration>5 and np.abs(total_loglik - last_loglik) < tol:\n",
        "            print(f\"Converged after {iteration + 1} iterations.\")\n",
        "            break\n",
        "\n",
        "        last_loglik = total_loglik\n",
        "        logliks.append(total_loglik)\n",
        "        print(f\"Iter {iteration + 1}: log-likelihood = {total_loglik:.4f}\")\n",
        "        print(\"T: \", torch.round(T, decimals=4))\n",
        "        print(\"E: \", torch.round(E, decimals=4))\n",
        "        print(\"pi: \", torch.round(pi, decimals=4))\n",
        "\n",
        "    return T, E, pi, logliks"
      ],
      "metadata": {
        "id": "4sHQ13s5g-_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1: Investigate the impact of parameter initialization on the algorithms convergence**\n",
        "\n",
        "1. **Generate observation sequences**  \n",
        "- Use a known HMM with specified parameters $(\\pi, T, E)$.\n",
        "$$\n",
        "  \\pi = [0.5, 0.5], \\quad\n",
        "  T = \\begin{bmatrix} 0.9 & 0.1 \\\\ 0.2 & 0.8 \\end{bmatrix}, \\quad\n",
        "  E = \\begin{bmatrix} 0.5 & 0.5 \\\\ 0.1 & 0.9 \\end{bmatrix}\n",
        "$$\n",
        "- Generate a set of 5 observation sequences from this model, each of length 500.\n",
        "\n",
        "2. **Examine the impact of initialization**  \n",
        "  - Run the Baum–Welch algorithm multiple times using\n",
        "    - 15 different random initial parameter values.  \n",
        "    - uniform initialization, e.g., all transition and emission probabilities equal and rows summing to 1:  \n",
        "$$\n",
        "    \\pi = [0.5, 0.5], \\quad\n",
        "    T = \\begin{bmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{bmatrix}, \\quad\n",
        "    E = \\begin{bmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{bmatrix}\n",
        "$$\n",
        "  - Create a line plot to:\n",
        "    - Compare how different initializations affect **convergence speed** and **final log-likelihood**.  \n",
        "    - Compare the calculated log-likelihoods to the log-likelihood of the data under the **true parameters** used to generate it.\n",
        "\n",
        "**Discussion Questions:**  \n",
        "- What effects do you observe from different initializations?  \n",
        "- How does the final convergence relate to the initial log-likelihood?  \n",
        "- Are there any special cases or patterns?  \n",
        "- Is uniform initialization a good strategy? Why or why not?\n",
        "- How can we initialize the algorithm to ensure \"good\" convergence?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ccyBIsFRB_bN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Posterior state probabilities and parameter evaluation**\n",
        "\n",
        "1. **Select learned parameters**  \n",
        "   - From your Baum–Welch runs, choose the parameter set $(\\hat{\\pi}, \\hat{T}, \\hat{E})$ that achieved the **highest log-likelihood** on the training sequence.\n",
        "\n",
        "2. **Generate a new sequence**  \n",
        "   - Using the **true model parameters** $(\\pi, T, E)$, generate a new observation sequence $X_{\\text{new}}$ and record the corresponding hidden states $Z_{\\text{true}}$.\n",
        "\n",
        "3. **Compute posterior state probabilities**  \n",
        "   - Using the selected learned parameters $(\\hat{\\pi}, \\hat{T}, \\hat{E})$, run the **forward–backward algorithm** to compute the posterior probabilities $\\gamma_n(i) = \\mathbb{P}(Z_n = i \\mid X_{\\text{new}}, \\hat{\\theta})$ for each time step.  \n",
        "   - Repeat the computation using the **true parameters** $(\\pi, T, E)$ for comparison.\n",
        "\n",
        "4. **Compare with true hidden states**  \n",
        "   - For each time step, compare the **most likely state** from $\\gamma_n$ to the true hidden state $Z_{\\text{true}}$.\n",
        "\n",
        "\n",
        "5. **Visualization suggestions**  \n",
        "   - Plot the **posterior probabilities over time** computed from learned vs. true parameters for each hidden state.  \n",
        "   - Overlay the **true hidden states** to visually assess how well the learned model captures the latent sequence.  \n",
        "\n",
        "How close are the posterior probabilities from the learned parameters to those from the true parameters?  \n",
        "Does the learned model recover the hidden states accurately?  \n",
        "Can you identify sequences or time steps where the model struggles?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eF7CfnyNkpuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3: Effect of sequence length on convergence**\n",
        "\n",
        "1. **Generate observation sequences**  \n",
        "   - Using the HMM from Exercise 1, generate multiple sequences of different lengths, e.g., 50, 100, and 1000.  \n",
        "   - Generate at least 5 sequences for each length.\n",
        "\n",
        "2. **Run Baum–Welch**  \n",
        "   - Apply the Baum–Welch algorithm separately to each sequence (i.e., train on one sequence at a time).\n",
        "\n",
        "3. **Analyze results**  \n",
        "   - For each sequence length, examine how the learned parameters and loglikelihoods vary between the algorithm runs.   \n",
        "   - Discuss how sequence length affects convergence speed, stability, and accuracy of parameter estimation.\n"
      ],
      "metadata": {
        "id": "mk0mZFxahjNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additional exercises:\n",
        "\n",
        "1. Repeat the analysis for a bigger model, eg. with 4 hidden states and 6 possible emissions. What are your observations?\n",
        "\n",
        "2. Partial parameter learning\n",
        "   - Assume the **emission matrix is known**. Modify the Baum–Welch algorithm to update only the **transition matrix** and **initial state probabilities**.  \n",
        "   - Investigate convergence behavior in this scenario and compare to learning all parameters simultaneously.  \n",
        "   - Discuss whether convergence is faster or more stable when fewer parameters are estimated.\n"
      ],
      "metadata": {
        "id": "B2CAi-WVe0kz"
      }
    }
  ]
}